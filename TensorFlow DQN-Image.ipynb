{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import os\n",
    "import datetime\n",
    "from gym import wrappers\n",
    "from collections import deque, Counter\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    img = tf.image.resize_with_pad(image,80,80)\n",
    "    img = tf.image.crop_to_bounding_box(img,9,9,70,70)\n",
    "    img = np.mean(img,axis=2)\n",
    "    return img\n",
    "def atleast_4d(x):\n",
    "    if x.ndim < 4:\n",
    "        y = tf.expand_dims(np.atleast_3d(x), axis=0)\n",
    "    else:\n",
    "        y = x\n",
    "    return y\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess the frame\n",
    "    frame = preprocess(state)\n",
    "    if is_new_episode:\n",
    "        # Element-wise max summation\n",
    "        maxframe = np.maximum(frame,frame)\n",
    "        # Append frame to deque\n",
    "        stacked_frames.append(maxframe)\n",
    "        stacked_frames.append(maxframe)\n",
    "        stacked_frames.append(maxframe)\n",
    "        stacked_frames.append(maxframe)\n",
    "        # Stack the frame\n",
    "        stacked_state = np.stack(stacked_frames,axis=2)\n",
    "    else:\n",
    "        maxframe=np.maximum(stacked_frames[-1],frame)\n",
    "        stacked_frames.append(maxframe)\n",
    "        stacked_state = np.stack(stacked_frames,axis=2)\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network class are built to define the network and implement forward pass manually\n",
    "class Network(tf.keras.Model):\n",
    "    def __init__(self, num_states, hidden_units, num_actions):\n",
    "        super(Network, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.InputLayer() # Input layers\n",
    "        self.flatten_layer = tf.keras.layers.Flatten()\n",
    "        self.conv = tf.keras.layers.Conv2D(32,(3,3),activation = 'relu',padding='same', kernel_initializer=VarianceScaling(scale=2.))\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64,(3,3),activation = 'relu',padding='same', kernel_initializer=VarianceScaling(scale=2.))\n",
    "        self.maxp = tf.keras.layers.MaxPooling2D((2,2),strides=2)\n",
    "        self.adv = tf.keras.layers.Dense(1,kernel_initializer=VarianceScaling(scale=2.))\n",
    "        self.val = tf.keras.layers.Dense(num_actions,kernel_initializer=VarianceScaling(scale=2.))\n",
    "        self.drop = tf.keras.layers.Dropout(0.2)\n",
    "        self.cussp = tf.keras.layers.Lambda(lambda w: tf.split(w,2,3))\n",
    "        self.redm = tf.keras.layers.Lambda(lambda w: tf.reduce_mean(w, axis=1, keepdims =True))\n",
    "        self.hidden_layers = [] # List of hidden layers\n",
    "        for i in hidden_units: # Create and append layers to the hidden layer list\n",
    "            self.hidden_layers.append(tf.keras.layers.Dense(\n",
    "            i,activation='relu',kernel_initializer='RandomNormal'))         \n",
    "#         self.output_layer = tf.keras.layers.Dense(\n",
    "#             num_actions,activation='linear', kernel_initializer = 'RandomNormal')\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs): # Forward passing\n",
    "#         inputs = preprocess(inputs)\n",
    "        z = self.input_layer(inputs) # Assign input to a layer\n",
    "        z = self.conv(z)\n",
    "        z = self.maxp(z)\n",
    "        z = self.conv2(z)\n",
    "        z = self.maxp(z)\n",
    "        z = self.drop(z)\n",
    "        for layer in self.hidden_layers: # Passes the input layer through all of the hidden layers\n",
    "            z = layer(z)\n",
    "        val_stream, adv_stream = self.cussp(z)\n",
    "        val_stream = self.flatten_layer(val_stream)\n",
    "        val = self.val(val_stream)\n",
    "        adv_stream = self.flatten_layer(adv_stream)\n",
    "        adv = self.adv(adv_stream)\n",
    "        q_val = tf.keras.layers.Add()([val, tf.keras.layers.Subtract()([adv, self.redm(adv)])])\n",
    "#         output = self.output_layer(z) # Return the output of the output layer\n",
    "        return q_val\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n",
    "        self.num_actions = num_actions # Number of action\n",
    "        self.gamma = gamma # Reward discount factor\n",
    "        self.max_experiences = max_experiences # Max number of exp\n",
    "        self.min_experiences = min_experiences # Min number of exp\n",
    "        self.experience = {'s': [], 'a':[], 'r':[], 's2': [],'done': []} # Exp holder\n",
    "        self.batch_size = batch_size # Batch size\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr) # Optimizer with learning rate\n",
    "        self.model = Network(num_states, hidden_units,num_actions) # Defining the model\n",
    "        \n",
    "    # Predict the next action with state as input\n",
    "    def predict(self,inputs):\n",
    "        return self.model(atleast_4d(inputs.astype('float32')))\n",
    "    \n",
    "    # Training the model\n",
    "    def train(self, TargetNet):\n",
    "        if len(self.experience['s']) < self.min_experiences: # If there are not enough experience to sample from\n",
    "            return 0\n",
    "        # Randomly sampling from experience (s,s',a,r)\n",
    "        ids = np.random.randint (low=0, high=len(self.experience['s']),size = self.batch_size)\n",
    "        states = np.asarray([self.experience['s'][i] for i in ids])\n",
    "        actions = np.asarray([self.experience['a'][i] for i in ids])\n",
    "        rewards = np.asarray([self.experience['r'][i] for i in ids])\n",
    "        states_next = np.asarray([self.experience['s2'][i] for i in ids])\n",
    "        \n",
    "        # Check for termial state \n",
    "        dones = np.asarray([self.experience['done'][i] for i in ids])\n",
    "        \n",
    "        # Predicting the value of the next state Q(s') by using predict and inputting the next state (s')\n",
    "        value_next = np.max(TargetNet.predict(states_next),axis =1)\n",
    "        actual_values = np.where(dones,rewards,rewards+self.gamma*value_next)\n",
    "\n",
    "        # Calculate the sqared loss between the real target and the prediction values\n",
    "        with tf.GradientTape() as tape: # Recording the computation to compute the differentialtion bakcward (backpropagation)\n",
    "            selected_action_values = tf.math.reduce_sum(\n",
    "                self.predict(states)* tf.one_hot(actions, self.num_actions), axis =1)\n",
    "            l = tf.keras.losses.Huber()\n",
    "            loss = l(actual_values , selected_action_values)\n",
    "        \n",
    "        # Backpropagation     \n",
    "        variables = self.model.trainable_variables # Call the weights of the model\n",
    "        gradients = tape.gradient(loss,variables) # Setup gradients \n",
    "        self.optimizer.apply_gradients(zip(gradients, variables)) # Doing backprop\n",
    "        return loss\n",
    "    \n",
    "    # Epsilon-Greedy Strategy\n",
    "    def get_action(self,states, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(self.num_actions) # Exploration\n",
    "        else:\n",
    "            return np.argmax(self.predict(np.atleast_2d(states))[0]) # Exploitation\n",
    "    \n",
    "    # Adding and pushing experience (Replay Memories)\n",
    "    def add_experience(self,exp):\n",
    "        if len(self.experience['s']) >= self.max_experiences:\n",
    "            for key in self.experience.keys():\n",
    "                self.experience[key].pop(0)\n",
    "                \n",
    "        for key,value in exp.items():\n",
    "            self.experience[key].append(value)\n",
    "            \n",
    "    # Copy weights from TrainNet to TargetNet\n",
    "    def copy_weights(self, TrainNet):\n",
    "        variables1 = self.model.trainable_variables\n",
    "        variables2 = TrainNet.model.trainable_variables\n",
    "        for v1, v2 in zip(variables1, variables2):\n",
    "            v1.assign(v2.numpy())\n",
    "    \n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iters = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    losses = list()\n",
    "    stack_size = 10\n",
    "    stacked_frames = deque([np.zeros((70,70,1),dtype=np.int)for i in range (stack_size)],maxlen =4)\n",
    "    observations,stacked_frames = stack_frames(stacked_frames,observations,True)\n",
    "    while not done:\n",
    "        action = TrainNet.get_action(observations, epsilon)\n",
    "        prev_observations = observations \n",
    "        observations, reward, done,_ = env.step(action)\n",
    "        observations,stacked_frames = stack_frames(stacked_frames,observations,False)\n",
    "        rewards += reward\n",
    "        if env.ale.lives() < 3:\n",
    "            done = True\n",
    "        if done:\n",
    "            reward = -200\n",
    "            env.reset()\n",
    "            \n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2':observations, 'done': done}\n",
    "        TrainNet.add_experience(exp)\n",
    "        loss = TrainNet.train(TargetNet)\n",
    "            \n",
    "        # Convert loss to int if loss is not int\n",
    "        if isinstance(loss,int):\n",
    "            losses.append(loss)\n",
    "        else:\n",
    "            losses.append(loss.numpy())\n",
    "            \n",
    "        iters += 1\n",
    "        if iters % copy_step == 0:\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "    return rewards, np.mean(losses)\n",
    "    \n",
    "def make_video(env,TrainNet):\n",
    "    env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"videos\"), force=True)\n",
    "    rewards = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    stack_size = 4\n",
    "    stacked_frames = deque([np.zeros((70,70,1),dtype=np.int)for i in range (stack_size)],maxlen =4)\n",
    "    observations,stacked_frames = stack_frames(stacked_frames,observations,True)\n",
    "    while not done:\n",
    "        action = TrainNet.get_action(observations,0)\n",
    "        observations, reward, done,_ = env.step(action)\n",
    "        observations,stacked_frames = stack_frames(stacked_frames,observations,False)\n",
    "        #env.reset()\n",
    "        steps += 1\n",
    "        rewards += reward\n",
    "    print(\"Testing steps: {} reward {}: \".format(steps,rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 episode reward: 15.0 eps: 1.0 avg reward (last 100): 15.0 episode loss:  0.0\n",
      "episode: 10 episode reward: 15.0 eps: 0.9990104948350412 avg reward (last 100): 59.54545454545455 episode loss:  0.0\n",
      "episode: 20 episode reward: 0.0 eps: 0.9980219786806598 avg reward (last 100): 53.63636363636363 episode loss:  0.0\n",
      "episode: 30 episode reward: 30.0 eps: 0.9970344505483393 avg reward (last 100): 40.45454545454545 episode loss:  0.0\n",
      "episode: 40 episode reward: 65.0 eps: 0.9960479094505515 avg reward (last 100): 35.45454545454545 episode loss:  1.0066488\n",
      "episode: 50 episode reward: 40.0 eps: 0.9950623544007555 avg reward (last 100): 60.90909090909091 episode loss:  0.9544299\n",
      "episode: 60 episode reward: 45.0 eps: 0.9940777844133959 avg reward (last 100): 76.36363636363636 episode loss:  1.0746521\n",
      "episode: 70 episode reward: 105.0 eps: 0.9930941985039028 avg reward (last 100): 51.36363636363637 episode loss:  0.80166435\n",
      "episode: 80 episode reward: 0.0 eps: 0.99211159568869 avg reward (last 100): 30.0 episode loss:  1.2605954\n",
      "episode: 90 episode reward: 0.0 eps: 0.9911299749851548 avg reward (last 100): 82.27272727272727 episode loss:  0.9966151\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "\n",
    "gamma = 0.5\n",
    "copy_step = 50\n",
    "num_states = len(env.observation_space.sample()) \n",
    "num_actions = env.action_space.n\n",
    "hidden_units = [100,100]\n",
    "max_experiences = 1000000\n",
    "min_experiences = 10000\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'logs/dqn/' + current_time\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "TrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences,batch_size, lr)\n",
    "TargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences,batch_size, lr)\n",
    "\n",
    "N = 1000 # number of episode\n",
    "total_rewards = np.empty(N)\n",
    "max_epsilon = 1\n",
    "decay = 0.0001\n",
    "min_epsilon = 0.01\n",
    "for n in range(N):\n",
    "    epsilon = min_epsilon + (max_epsilon-min_epsilon) * np.exp(-decay*n)\n",
    "    total_reward, losses = play_game(env,TrainNet,TargetNet,epsilon,copy_step)\n",
    "    total_rewards[n] = total_reward\n",
    "    avg_rewards = total_rewards[max(0,n-10):(n+1)].mean()\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('episode reward', total_reward, step = n)\n",
    "        tf.summary.scalar('running avg reward(10)', avg_rewards, step =n)\n",
    "        tf.summary.scalar('average loss', losses, step=n)\n",
    "    if n%10 ==0:\n",
    "         print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards,\n",
    "                  \"episode loss: \", losses)\n",
    "\n",
    "        \n",
    "print(\"avg reward for last 10 episodes:\", avg_rewards)\n",
    "make_video(env, TrainNet)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
