{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import os\n",
    "import datetime\n",
    "from gym import wrappers\n",
    "from collections import deque, Counter\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    img = tf.image.resize_with_pad(image,80,80)\n",
    "    img = tf.image.crop_to_bounding_box(img,9,9,70,70)\n",
    "    img = np.mean(img,axis=2)\n",
    "    return img\n",
    "def atleast_4d(x):\n",
    "    if x.ndim < 4:\n",
    "        y = tf.expand_dims(np.atleast_3d(x), axis=0)\n",
    "    else:\n",
    "        y = x\n",
    "    return y\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    # Preprocess the frame\n",
    "    frame = preprocess(state)\n",
    "    if is_new_episode:\n",
    "        # Element-wise max summation\n",
    "        maxframe = np.maximum(frame,frame)\n",
    "        # Append frame to deque\n",
    "        stacked_frames.append(maxframe)\n",
    "        stacked_frames.append(maxframe)\n",
    "        stacked_frames.append(maxframe)\n",
    "        stacked_frames.append(maxframe)\n",
    "        # Stack the frame\n",
    "        stacked_state = np.stack(stacked_frames,axis=2)\n",
    "    else:\n",
    "        maxframe=np.maximum(stacked_frames[-1],frame)\n",
    "        stacked_frames.append(maxframe)\n",
    "        stacked_state = np.stack(stacked_frames,axis=2)\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network class are built to define the network and implement forward pass manually\n",
    "class Network(tf.keras.Model):\n",
    "    def __init__(self, num_states, hidden_units, num_actions):\n",
    "        super(Network, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.InputLayer() # Input layers\n",
    "        self.flatten_layer = tf.keras.layers.Flatten()\n",
    "        self.conv = tf.keras.layers.Conv2D(32,(3,3),activation = 'relu',padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64,(3,3),activation = 'relu',padding='same')\n",
    "        self.maxp = tf.keras.layers.MaxPooling2D((2,2),strides=2)\n",
    "        self.hidd = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.drop = tf.keras.layers.Dropout(0.2)\n",
    "        self.hidden_layers = [] # List of hidden layers\n",
    "        for i in hidden_units: # Create and append layers to the hidden layer list\n",
    "            self.hidden_layers.append(tf.keras.layers.Dense(\n",
    "            i,activation='relu',kernel_initializer='RandomNormal'))         \n",
    "        self.output_layer = tf.keras.layers.Dense(\n",
    "            num_actions,activation='linear', kernel_initializer = 'RandomNormal')\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, inputs): # Forward passing\n",
    "#         inputs = preprocess(inputs)\n",
    "        z = self.input_layer(inputs) # Assign input to a layer\n",
    "        z = self.conv(z)\n",
    "        z = self.maxp(z)\n",
    "        z = self.conv2(z)\n",
    "        z = self.maxp(z)\n",
    "        z = self.drop(z)\n",
    "        z = self.flatten_layer(z)\n",
    "        z = self.hidd(z)\n",
    "        for layer in self.hidden_layers: # Passes the input layer through all of the hidden layers\n",
    "            z = layer(z)\n",
    "        output = self.output_layer(z) # Return the output of the output layer\n",
    "        return output\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n",
    "        self.num_actions = num_actions # Number of action\n",
    "        self.gamma = gamma # Reward discount factor\n",
    "        self.max_experiences = max_experiences # Max number of exp\n",
    "        self.min_experiences = min_experiences # Min number of exp\n",
    "        self.experience = {'s': [], 'a':[], 'r':[], 's2': [],'done': []} # Exp holder\n",
    "        self.batch_size = batch_size # Batch size\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr) # Optimizer with learning rate\n",
    "        self.model = Network(num_states, hidden_units,num_actions) # Defining the model\n",
    "        \n",
    "    # Predict the next action with state as input\n",
    "    def predict(self,inputs):\n",
    "        return self.model(atleast_4d(inputs.astype('float32')))\n",
    "    \n",
    "    # Training the model\n",
    "    def train(self, TargetNet):\n",
    "        if len(self.experience['s']) < self.min_experiences: # If there are not enough experience to sample from\n",
    "            return 0\n",
    "        # Randomly sampling from experience (s,s',a,r)\n",
    "        ids = np.random.randint (low=0, high=len(self.experience['s']),size = self.batch_size)\n",
    "        states = np.asarray([self.experience['s'][i] for i in ids])\n",
    "        actions = np.asarray([self.experience['a'][i] for i in ids])\n",
    "        rewards = np.asarray([self.experience['r'][i] for i in ids])\n",
    "        states_next = np.asarray([self.experience['s2'][i] for i in ids])\n",
    "        \n",
    "        # Check for termial state \n",
    "        dones = np.asarray([self.experience['done'][i] for i in ids])\n",
    "        \n",
    "        # Predicting the value of the next state Q(s') by using predict and inputting the next state (s')\n",
    "        value_next = np.max(TargetNet.predict(states_next),axis =1)\n",
    "        actual_values = np.where(dones,rewards,rewards+self.gamma*value_next)\n",
    "\n",
    "        # Calculate the sqared loss between the real target and the prediction values\n",
    "        with tf.GradientTape() as tape: # Recording the computation to compute the differentialtion bakcward (backpropagation)\n",
    "            selected_action_values = tf.math.reduce_sum(\n",
    "                self.predict(states)* tf.one_hot(actions, self.num_actions), axis =1)\n",
    "            l = tf.keras.losses.Huber()\n",
    "            loss = l(actual_values , selected_action_values)\n",
    "        \n",
    "        # Backpropagation     \n",
    "        variables = self.model.trainable_variables # Call the weights of the model\n",
    "        gradients = tape.gradient(loss,variables) # Setup gradients \n",
    "        self.optimizer.apply_gradients(zip(gradients, variables)) # Doing backprop\n",
    "        return loss\n",
    "    \n",
    "    # Epsilon-Greedy Strategy\n",
    "    def get_action(self,states, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(self.num_actions) # Exploration\n",
    "        else:\n",
    "            return np.argmax(self.predict(np.atleast_2d(states))[0]) # Exploitation\n",
    "    \n",
    "    # Adding and pushing experience (Replay Memories)\n",
    "    def add_experience(self,exp):\n",
    "        if len(self.experience['s']) >= self.max_experiences:\n",
    "            for key in self.experience.keys():\n",
    "                self.experience[key].pop(0)\n",
    "                \n",
    "        for key,value in exp.items():\n",
    "            self.experience[key].append(value)\n",
    "            \n",
    "    # Copy weights from TrainNet to TargetNet\n",
    "    def copy_weights(self, TrainNet):\n",
    "        variables1 = self.model.trainable_variables\n",
    "        variables2 = TrainNet.model.trainable_variables\n",
    "        for v1, v2 in zip(variables1, variables2):\n",
    "            v1.assign(v2.numpy())\n",
    "    \n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iters = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    losses = list()\n",
    "    stack_size = 8\n",
    "    stacked_frames = deque([np.zeros((70,70,1),dtype=np.int)for i in range (stack_size)],maxlen =4)\n",
    "    observations,stacked_frames = stack_frames(stacked_frames,observations,True)\n",
    "    while not done:\n",
    "        action = TrainNet.get_action(observations, epsilon)\n",
    "        prev_observations = observations \n",
    "        observations, reward, done,_ = env.step(action)\n",
    "        observations,stacked_frames = stack_frames(stacked_frames,observations,False)\n",
    "        rewards += reward\n",
    "        if env.ale.lives() < 3:\n",
    "            done = True\n",
    "        if done:\n",
    "            reward = -200\n",
    "            env.reset()\n",
    "            \n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2':observations, 'done': done}\n",
    "        TrainNet.add_experience(exp)\n",
    "        loss = TrainNet.train(TargetNet)\n",
    "            \n",
    "        # Convert loss to int if loss is not int\n",
    "        if isinstance(loss,int):\n",
    "            losses.append(loss)\n",
    "        else:\n",
    "            losses.append(loss.numpy())\n",
    "            \n",
    "        iters += 1\n",
    "        if iters % copy_step == 0:\n",
    "            TargetNet.copy_weights(TrainNet)\n",
    "    return rewards, np.mean(losses)\n",
    "    \n",
    "def make_video(env,TrainNet):\n",
    "    env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"videos\"), force=True)\n",
    "    rewards = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    stack_size = 4\n",
    "    stacked_frames = deque([np.zeros((70,70,1),dtype=np.int)for i in range (stack_size)],maxlen =4)\n",
    "    observations,stacked_frames = stack_frames(stacked_frames,observations,True)\n",
    "    while not done:\n",
    "        action = TrainNet.get_action(observations,0)\n",
    "        observations, reward, done,_ = env.step(action)\n",
    "        observations,stacked_frames = stack_frames(stacked_frames,observations,False)\n",
    "        #env.reset()\n",
    "        steps += 1\n",
    "        rewards += reward\n",
    "    print(\"Testing steps: {} reward {}: \".format(steps,rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 episode reward: 250.0 eps: 1.0 avg reward (last 100): 250.0 episode loss:  0.0\n",
      "episode: 10 episode reward: 105.0 eps: 0.9990104948350412 avg reward (last 100): 56.81818181818182 episode loss:  0.0\n",
      "episode: 20 episode reward: 0.0 eps: 0.9980219786806598 avg reward (last 100): 51.81818181818182 episode loss:  0.0\n",
      "episode: 30 episode reward: 0.0 eps: 0.9970344505483393 avg reward (last 100): 51.36363636363637 episode loss:  0.0\n",
      "episode: 40 episode reward: 55.0 eps: 0.9960479094505515 avg reward (last 100): 75.0 episode loss:  0.883912\n",
      "episode: 50 episode reward: 35.0 eps: 0.9950623544007555 avg reward (last 100): 37.72727272727273 episode loss:  0.873517\n",
      "episode: 60 episode reward: 45.0 eps: 0.9940777844133959 avg reward (last 100): 35.90909090909091 episode loss:  0.90429336\n",
      "episode: 70 episode reward: 55.0 eps: 0.9930941985039028 avg reward (last 100): 33.63636363636363 episode loss:  1.0163525\n",
      "episode: 80 episode reward: 20.0 eps: 0.99211159568869 avg reward (last 100): 50.45454545454545 episode loss:  0.9449389\n",
      "episode: 90 episode reward: 35.0 eps: 0.9911299749851548 avg reward (last 100): 65.9090909090909 episode loss:  0.88861954\n",
      "episode: 100 episode reward: 310.0 eps: 0.9901493354116764 avg reward (last 100): 69.54545454545455 episode loss:  0.858606\n",
      "episode: 110 episode reward: 30.0 eps: 0.9891696759876151 avg reward (last 100): 105.0 episode loss:  1.0146774\n",
      "episode: 120 episode reward: 15.0 eps: 0.9881909957333113 avg reward (last 100): 32.27272727272727 episode loss:  0.83141637\n",
      "episode: 130 episode reward: 40.0 eps: 0.9872132936700847 avg reward (last 100): 50.0 episode loss:  0.91443527\n",
      "episode: 140 episode reward: 5.0 eps: 0.9862365688202333 avg reward (last 100): 62.27272727272727 episode loss:  1.0742044\n",
      "episode: 150 episode reward: 30.0 eps: 0.985260820207032 avg reward (last 100): 45.90909090909091 episode loss:  0.78101504\n",
      "episode: 160 episode reward: 50.0 eps: 0.9842860468547323 avg reward (last 100): 35.45454545454545 episode loss:  0.81004786\n",
      "episode: 170 episode reward: 65.0 eps: 0.9833122477885605 avg reward (last 100): 46.36363636363637 episode loss:  1.1012437\n",
      "episode: 180 episode reward: 55.0 eps: 0.9823394220347178 avg reward (last 100): 56.36363636363637 episode loss:  0.94301754\n",
      "episode: 190 episode reward: 0.0 eps: 0.9813675686203779 avg reward (last 100): 26.818181818181817 episode loss:  0.8162511\n",
      "episode: 200 episode reward: 55.0 eps: 0.9803966865736877 avg reward (last 100): 60.0 episode loss:  0.8672951\n",
      "episode: 210 episode reward: 140.0 eps: 0.979426774923765 avg reward (last 100): 44.09090909090909 episode loss:  0.87401474\n",
      "episode: 220 episode reward: 30.0 eps: 0.978457832700698 avg reward (last 100): 85.9090909090909 episode loss:  0.81554943\n",
      "episode: 230 episode reward: 15.0 eps: 0.9774898589355443 avg reward (last 100): 35.90909090909091 episode loss:  1.071442\n",
      "episode: 240 episode reward: 15.0 eps: 0.9765228526603302 avg reward (last 100): 61.36363636363637 episode loss:  0.9059104\n",
      "episode: 250 episode reward: 30.0 eps: 0.9755568129080493 avg reward (last 100): 60.45454545454545 episode loss:  0.69728386\n",
      "episode: 260 episode reward: 30.0 eps: 0.9745917387126619 avg reward (last 100): 53.63636363636363 episode loss:  0.82225996\n",
      "episode: 270 episode reward: 110.0 eps: 0.9736276291090934 avg reward (last 100): 67.72727272727273 episode loss:  0.85730875\n",
      "episode: 280 episode reward: 110.0 eps: 0.9726644831332344 avg reward (last 100): 48.18181818181818 episode loss:  1.0239031\n",
      "episode: 290 episode reward: 55.0 eps: 0.9717022998219388 avg reward (last 100): 35.45454545454545 episode loss:  1.1035497\n",
      "episode: 300 episode reward: 30.0 eps: 0.970741078213023 avg reward (last 100): 51.81818181818182 episode loss:  0.8229009\n",
      "episode: 310 episode reward: 50.0 eps: 0.9697808173452657 avg reward (last 100): 71.81818181818181 episode loss:  0.88524836\n",
      "episode: 320 episode reward: 15.0 eps: 0.9688215162584056 avg reward (last 100): 44.54545454545455 episode loss:  0.9633457\n",
      "episode: 330 episode reward: 15.0 eps: 0.9678631739931417 avg reward (last 100): 121.81818181818181 episode loss:  0.7946757\n",
      "episode: 340 episode reward: 75.0 eps: 0.9669057895911316 avg reward (last 100): 76.36363636363636 episode loss:  0.74641174\n",
      "episode: 350 episode reward: 30.0 eps: 0.9659493620949908 avg reward (last 100): 44.54545454545455 episode loss:  0.98232204\n",
      "episode: 360 episode reward: 5.0 eps: 0.9649938905482919 avg reward (last 100): 46.36363636363637 episode loss:  0.7517605\n",
      "episode: 370 episode reward: 15.0 eps: 0.9640393739955629 avg reward (last 100): 39.54545454545455 episode loss:  0.8531993\n",
      "episode: 380 episode reward: 35.0 eps: 0.9630858114822876 avg reward (last 100): 63.63636363636363 episode loss:  0.78567094\n",
      "episode: 390 episode reward: 5.0 eps: 0.962133202054903 avg reward (last 100): 82.27272727272727 episode loss:  0.7467513\n",
      "episode: 400 episode reward: 45.0 eps: 0.9611815447608 avg reward (last 100): 75.0 episode loss:  0.8324833\n",
      "episode: 410 episode reward: 15.0 eps: 0.9602308386483209 avg reward (last 100): 31.363636363636363 episode loss:  0.78236604\n",
      "episode: 420 episode reward: 5.0 eps: 0.9592810827667597 avg reward (last 100): 36.81818181818182 episode loss:  0.80312407\n",
      "episode: 430 episode reward: 30.0 eps: 0.9583322761663603 avg reward (last 100): 45.0 episode loss:  0.73982644\n",
      "episode: 440 episode reward: 30.0 eps: 0.9573844178983162 avg reward (last 100): 101.36363636363636 episode loss:  0.91392547\n",
      "episode: 450 episode reward: 70.0 eps: 0.9564375070147688 avg reward (last 100): 44.54545454545455 episode loss:  0.8559706\n",
      "episode: 460 episode reward: 110.0 eps: 0.9554915425688075 avg reward (last 100): 79.54545454545455 episode loss:  0.81664526\n",
      "episode: 470 episode reward: 50.0 eps: 0.9545465236144673 avg reward (last 100): 45.0 episode loss:  0.9119525\n",
      "episode: 480 episode reward: 0.0 eps: 0.9536024492067297 avg reward (last 100): 81.81818181818181 episode loss:  0.95355165\n",
      "episode: 490 episode reward: 15.0 eps: 0.9526593184015199 avg reward (last 100): 55.90909090909091 episode loss:  0.71649235\n",
      "episode: 500 episode reward: 10.0 eps: 0.9517171302557069 avg reward (last 100): 30.454545454545453 episode loss:  0.9815004\n",
      "episode: 510 episode reward: 50.0 eps: 0.9507758838271028 avg reward (last 100): 82.27272727272727 episode loss:  0.9276533\n",
      "episode: 520 episode reward: 35.0 eps: 0.9498355781744606 avg reward (last 100): 41.36363636363637 episode loss:  0.9765282\n",
      "episode: 530 episode reward: 50.0 eps: 0.9488962123574752 avg reward (last 100): 67.27272727272727 episode loss:  1.0297701\n",
      "episode: 540 episode reward: 50.0 eps: 0.9479577854367803 avg reward (last 100): 52.27272727272727 episode loss:  0.9247488\n",
      "episode: 550 episode reward: 0.0 eps: 0.947020296473949 avg reward (last 100): 45.45454545454545 episode loss:  0.8416927\n",
      "episode: 560 episode reward: 70.0 eps: 0.9460837445314924 avg reward (last 100): 64.0909090909091 episode loss:  0.83397174\n",
      "episode: 570 episode reward: 35.0 eps: 0.9451481286728581 avg reward (last 100): 57.72727272727273 episode loss:  0.96862185\n",
      "episode: 580 episode reward: 20.0 eps: 0.9442134479624306 avg reward (last 100): 37.27272727272727 episode loss:  0.87623453\n",
      "episode: 590 episode reward: 50.0 eps: 0.9432797014655288 avg reward (last 100): 48.63636363636363 episode loss:  0.8654673\n",
      "episode: 600 episode reward: 160.0 eps: 0.9423468882484062 avg reward (last 100): 51.36363636363637 episode loss:  0.92294097\n",
      "episode: 610 episode reward: 0.0 eps: 0.9414150073782496 avg reward (last 100): 88.63636363636364 episode loss:  0.85781544\n",
      "episode: 620 episode reward: 5.0 eps: 0.940484057923178 avg reward (last 100): 53.18181818181818 episode loss:  0.8217147\n",
      "episode: 630 episode reward: 50.0 eps: 0.9395540389522419 avg reward (last 100): 35.0 episode loss:  0.8797559\n",
      "episode: 640 episode reward: 35.0 eps: 0.9386249495354222 avg reward (last 100): 27.727272727272727 episode loss:  0.92258865\n",
      "episode: 650 episode reward: 50.0 eps: 0.9376967887436294 avg reward (last 100): 50.0 episode loss:  0.8551222\n",
      "episode: 660 episode reward: 65.0 eps: 0.9367695556487027 avg reward (last 100): 31.818181818181817 episode loss:  0.98861647\n",
      "episode: 670 episode reward: 35.0 eps: 0.9358432493234088 avg reward (last 100): 56.81818181818182 episode loss:  0.9037451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 680 episode reward: 0.0 eps: 0.9349178688414413 avg reward (last 100): 28.636363636363637 episode loss:  0.70204604\n",
      "episode: 690 episode reward: 40.0 eps: 0.9339934132774199 avg reward (last 100): 41.36363636363637 episode loss:  0.94469845\n",
      "episode: 700 episode reward: 0.0 eps: 0.9330698817068888 avg reward (last 100): 48.18181818181818 episode loss:  0.734093\n",
      "episode: 710 episode reward: 70.0 eps: 0.9321472732063162 avg reward (last 100): 47.72727272727273 episode loss:  0.8665257\n",
      "episode: 720 episode reward: 0.0 eps: 0.9312255868530936 avg reward (last 100): 51.36363636363637 episode loss:  0.7603725\n",
      "episode: 730 episode reward: 95.0 eps: 0.9303048217255349 avg reward (last 100): 35.90909090909091 episode loss:  0.9448981\n",
      "episode: 740 episode reward: 30.0 eps: 0.9293849769028744 avg reward (last 100): 54.09090909090909 episode loss:  1.0168904\n",
      "episode: 750 episode reward: 50.0 eps: 0.9284660514652673 avg reward (last 100): 47.72727272727273 episode loss:  0.9434723\n",
      "episode: 760 episode reward: 95.0 eps: 0.9275480444937885 avg reward (last 100): 59.54545454545455 episode loss:  1.0691181\n",
      "episode: 770 episode reward: 110.0 eps: 0.9266309550704304 avg reward (last 100): 84.54545454545455 episode loss:  0.87321717\n",
      "episode: 780 episode reward: 50.0 eps: 0.9257147822781039 avg reward (last 100): 98.63636363636364 episode loss:  0.8887174\n",
      "episode: 790 episode reward: 40.0 eps: 0.9247995252006359 avg reward (last 100): 49.54545454545455 episode loss:  1.0102516\n",
      "episode: 800 episode reward: 45.0 eps: 0.9238851829227694 avg reward (last 100): 65.0 episode loss:  1.1013683\n",
      "episode: 810 episode reward: 10.0 eps: 0.922971754530162 avg reward (last 100): 41.81818181818182 episode loss:  0.764958\n",
      "episode: 820 episode reward: 5.0 eps: 0.9220592391093853 avg reward (last 100): 60.0 episode loss:  1.0322068\n",
      "episode: 830 episode reward: 110.0 eps: 0.9211476357479235 avg reward (last 100): 50.90909090909091 episode loss:  0.848866\n",
      "episode: 840 episode reward: 70.0 eps: 0.9202369435341734 avg reward (last 100): 62.72727272727273 episode loss:  0.88207823\n",
      "episode: 850 episode reward: 30.0 eps: 0.9193271615574428 avg reward (last 100): 49.09090909090909 episode loss:  1.103855\n",
      "episode: 860 episode reward: 15.0 eps: 0.9184182889079494 avg reward (last 100): 30.454545454545453 episode loss:  0.58357084\n",
      "episode: 870 episode reward: 305.0 eps: 0.9175103246768207 avg reward (last 100): 73.18181818181819 episode loss:  0.7545422\n",
      "episode: 880 episode reward: 15.0 eps: 0.9166032679560924 avg reward (last 100): 92.72727272727273 episode loss:  0.5052869\n",
      "episode: 890 episode reward: 30.0 eps: 0.9156971178387074 avg reward (last 100): 48.63636363636363 episode loss:  0.88844836\n",
      "episode: 900 episode reward: 65.0 eps: 0.9147918734185159 avg reward (last 100): 50.90909090909091 episode loss:  0.8578247\n",
      "episode: 910 episode reward: 30.0 eps: 0.9138875337902731 avg reward (last 100): 55.45454545454545 episode loss:  1.0377203\n",
      "episode: 920 episode reward: 50.0 eps: 0.9129840980496394 avg reward (last 100): 43.18181818181818 episode loss:  0.805905\n",
      "episode: 930 episode reward: 35.0 eps: 0.9120815652931792 avg reward (last 100): 45.45454545454545 episode loss:  1.0768496\n",
      "episode: 940 episode reward: 50.0 eps: 0.9111799346183593 avg reward (last 100): 87.72727272727273 episode loss:  0.9619559\n",
      "episode: 950 episode reward: 15.0 eps: 0.9102792051235491 avg reward (last 100): 57.72727272727273 episode loss:  0.9305378\n",
      "episode: 960 episode reward: 15.0 eps: 0.9093793759080191 avg reward (last 100): 29.545454545454547 episode loss:  1.0608443\n",
      "episode: 970 episode reward: 15.0 eps: 0.90848044607194 avg reward (last 100): 105.0 episode loss:  0.8774872\n",
      "episode: 980 episode reward: 40.0 eps: 0.9075824147163817 avg reward (last 100): 64.54545454545455 episode loss:  1.1479107\n",
      "episode: 990 episode reward: 0.0 eps: 0.906685280943313 avg reward (last 100): 70.9090909090909 episode loss:  0.89861757\n",
      "episode: 1000 episode reward: 80.0 eps: 0.9057890438555999 avg reward (last 100): 24.09090909090909 episode loss:  0.7271537\n",
      "episode: 1010 episode reward: 30.0 eps: 0.9048937025570055 avg reward (last 100): 78.63636363636364 episode loss:  1.0998272\n",
      "episode: 1020 episode reward: 45.0 eps: 0.903999256152188 avg reward (last 100): 35.90909090909091 episode loss:  0.9881019\n",
      "episode: 1030 episode reward: 15.0 eps: 0.9031057037467013 avg reward (last 100): 60.0 episode loss:  0.8198762\n",
      "episode: 1040 episode reward: 90.0 eps: 0.9022130444469927 avg reward (last 100): 109.54545454545455 episode loss:  1.0595046\n",
      "episode: 1050 episode reward: 80.0 eps: 0.9013212773604029 avg reward (last 100): 69.54545454545455 episode loss:  0.9094133\n",
      "episode: 1060 episode reward: 0.0 eps: 0.9004304015951649 avg reward (last 100): 49.09090909090909 episode loss:  1.0662158\n",
      "episode: 1070 episode reward: 35.0 eps: 0.8995404162604025 avg reward (last 100): 74.0909090909091 episode loss:  1.0989183\n",
      "episode: 1080 episode reward: 285.0 eps: 0.8986513204661305 avg reward (last 100): 66.81818181818181 episode loss:  1.0153557\n",
      "episode: 1090 episode reward: 5.0 eps: 0.8977631133232531 avg reward (last 100): 60.90909090909091 episode loss:  1.1399435\n",
      "episode: 1100 episode reward: 65.0 eps: 0.896875793943563 avg reward (last 100): 75.0 episode loss:  1.088635\n",
      "episode: 1110 episode reward: 75.0 eps: 0.8959893614397407 avg reward (last 100): 50.90909090909091 episode loss:  1.0041803\n",
      "episode: 1120 episode reward: 50.0 eps: 0.8951038149253536 avg reward (last 100): 32.72727272727273 episode loss:  1.0922567\n",
      "episode: 1130 episode reward: 50.0 eps: 0.8942191535148554 avg reward (last 100): 52.72727272727273 episode loss:  0.9741659\n",
      "episode: 1140 episode reward: 50.0 eps: 0.8933353763235842 avg reward (last 100): 81.81818181818181 episode loss:  0.94159704\n",
      "episode: 1150 episode reward: 110.0 eps: 0.892452482467763 avg reward (last 100): 85.9090909090909 episode loss:  1.1635183\n",
      "episode: 1160 episode reward: 5.0 eps: 0.8915704710644979 avg reward (last 100): 40.45454545454545 episode loss:  0.78825545\n",
      "episode: 1170 episode reward: 35.0 eps: 0.8906893412317772 avg reward (last 100): 45.0 episode loss:  0.95189214\n",
      "episode: 1180 episode reward: 15.0 eps: 0.8898090920884711 avg reward (last 100): 35.45454545454545 episode loss:  0.8434689\n",
      "episode: 1190 episode reward: 50.0 eps: 0.8889297227543306 avg reward (last 100): 56.81818181818182 episode loss:  0.7733348\n",
      "episode: 1200 episode reward: 30.0 eps: 0.888051232349986 avg reward (last 100): 70.0 episode loss:  1.0537623\n",
      "episode: 1210 episode reward: 0.0 eps: 0.8871736199969468 avg reward (last 100): 59.09090909090909 episode loss:  0.91128933\n",
      "episode: 1220 episode reward: 20.0 eps: 0.8862968848176008 avg reward (last 100): 35.45454545454545 episode loss:  0.8653696\n",
      "episode: 1230 episode reward: 45.0 eps: 0.8854210259352127 avg reward (last 100): 55.90909090909091 episode loss:  0.7643399\n",
      "episode: 1240 episode reward: 15.0 eps: 0.8845460424739234 avg reward (last 100): 51.81818181818182 episode loss:  0.7863889\n",
      "episode: 1250 episode reward: 75.0 eps: 0.8836719335587495 avg reward (last 100): 62.27272727272727 episode loss:  1.0267444\n",
      "episode: 1260 episode reward: 15.0 eps: 0.8827986983155819 avg reward (last 100): 62.72727272727273 episode loss:  1.0516722\n",
      "episode: 1270 episode reward: 35.0 eps: 0.8819263358711854 avg reward (last 100): 55.45454545454545 episode loss:  0.5257069\n",
      "episode: 1280 episode reward: 5.0 eps: 0.8810548453531973 avg reward (last 100): 24.545454545454547 episode loss:  1.0157198\n",
      "episode: 1290 episode reward: 65.0 eps: 0.8801842258901273 avg reward (last 100): 43.18181818181818 episode loss:  0.7881747\n",
      "episode: 1300 episode reward: 75.0 eps: 0.8793144766113556 avg reward (last 100): 51.81818181818182 episode loss:  0.96155304\n",
      "episode: 1310 episode reward: 5.0 eps: 0.8784455966471331 avg reward (last 100): 42.27272727272727 episode loss:  0.99698234\n",
      "episode: 1320 episode reward: 30.0 eps: 0.8775775851285795 avg reward (last 100): 40.45454545454545 episode loss:  0.84088725\n",
      "episode: 1330 episode reward: 105.0 eps: 0.8767104411876834 avg reward (last 100): 41.81818181818182 episode loss:  1.0150506\n",
      "episode: 1340 episode reward: 30.0 eps: 0.8758441639573007 avg reward (last 100): 47.72727272727273 episode loss:  0.93121636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1350 episode reward: 0.0 eps: 0.8749787525711541 avg reward (last 100): 43.63636363636363 episode loss:  0.9606236\n",
      "episode: 1360 episode reward: 10.0 eps: 0.8741142061638321 avg reward (last 100): 43.63636363636363 episode loss:  0.5918446\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.39 MiB for an array with shape (32, 70, 70, 4) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7f82737b7489>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_epsilon\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_epsilon\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmin_epsilon\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdecay\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mtotal_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrainNet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTargetNet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcopy_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mavg_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-9054712bda0b>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(env, TrainNet, TargetNet, epsilon, copy_step)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m's'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprev_observations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m's2'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'done'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mTrainNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_experience\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTargetNet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Convert loss to int if loss is not int\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-3974fe09348f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, TargetNet)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Predicting the value of the next state Q(s') by using predict and inputting the next state (s')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mvalue_next\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTargetNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates_next\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mactual_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvalue_next\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-3974fe09348f>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Predict the next action with state as input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0matleast_4d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# Training the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.39 MiB for an array with shape (32, 70, 70, 4) and data type float32"
     ]
    }
   ],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "\n",
    "gamma = 0.5\n",
    "copy_step = 50\n",
    "num_states = len(env.observation_space.sample()) \n",
    "num_actions = env.action_space.n\n",
    "hidden_units = [100,100]\n",
    "max_experiences = 1000000\n",
    "min_experiences = 10000\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = 'logs/dqn/' + current_time\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "TrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences,batch_size, lr)\n",
    "TargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences,batch_size, lr)\n",
    "\n",
    "N = 10000 # number of episode\n",
    "total_rewards = np.empty(N)\n",
    "max_epsilon = 1\n",
    "decay = 0.0001\n",
    "min_epsilon = 0.01\n",
    "for n in range(N):\n",
    "    epsilon = min_epsilon + (max_epsilon-min_epsilon) * np.exp(-decay*n)\n",
    "    total_reward, losses = play_game(env,TrainNet,TargetNet,epsilon,copy_step)\n",
    "    total_rewards[n] = total_reward\n",
    "    avg_rewards = total_rewards[max(0,n-10):(n+1)].mean()\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('episode reward', total_reward, step = n)\n",
    "        tf.summary.scalar('running avg reward(10)', avg_rewards, step =n)\n",
    "        tf.summary.scalar('average loss', losses, step=n)\n",
    "    if n%10 ==0:\n",
    "         print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards,\n",
    "                  \"episode loss: \", losses)\n",
    "\n",
    "        \n",
    "print(\"avg reward for last 10 episodes:\", avg_rewards)\n",
    "make_video(env, TrainNet)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
