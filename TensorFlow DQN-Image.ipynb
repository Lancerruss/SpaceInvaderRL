{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"TensorFlow DQN-Image.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"hTdESUAt3BYS","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","import gym\n","import os\n","import datetime\n","from gym import wrappers\n","from collections import deque, Counter\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow import keras\n","from tensorflow.keras.initializers import VarianceScaling\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VR9bE2jv3BYV","colab_type":"code","colab":{}},"source":["def preprocess(image):\n","    img = tf.image.resize_with_pad(image,80,80)\n","    img = tf.image.crop_to_bounding_box(img,9,9,70,70)\n","    img = np.mean(img,axis=2)\n","    return img\n","def atleast_4d(x):\n","    if x.ndim < 4:\n","        y = tf.expand_dims(np.atleast_3d(x), axis=0)\n","    else:\n","        y = x\n","    return y\n","\n","def stack_frames(stacked_frames, state, is_new_episode):\n","    # Preprocess the frame\n","    frame = preprocess(state)\n","    if is_new_episode:\n","        # Element-wise max summation\n","        maxframe = np.maximum(frame,frame)\n","        # Append frame to deque\n","        stacked_frames.append(maxframe)\n","        stacked_frames.append(maxframe)\n","        stacked_frames.append(maxframe)\n","        stacked_frames.append(maxframe)\n","        # Stack the frame\n","        stacked_state = np.stack(stacked_frames,axis=2)\n","    else:\n","        maxframe=np.maximum(stacked_frames[-1],frame)\n","        stacked_frames.append(maxframe)\n","        stacked_state = np.stack(stacked_frames,axis=2)\n","    return stacked_state, stacked_frames"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eRgkfaek3BYX","colab_type":"text"},"source":["# Building Network Class"]},{"cell_type":"code","metadata":{"id":"1X9YPwaj3BYY","colab_type":"code","colab":{}},"source":["# Network class are built to define the network and implement forward pass manually\n","class Network(tf.keras.Model):\n","    def __init__(self, num_states, hidden_units, num_actions):\n","        super(Network, self).__init__()\n","        self.input_layer = tf.keras.layers.InputLayer() # Input layers\n","        self.flatten_layer = tf.keras.layers.Flatten()\n","        self.conv = tf.keras.layers.Conv2D(32,(3,3),activation = 'relu',padding='same', kernel_initializer=VarianceScaling(scale=2.))\n","        self.conv2 = tf.keras.layers.Conv2D(64,(3,3),activation = 'relu',padding='same', kernel_initializer=VarianceScaling(scale=2.))\n","        self.maxp = tf.keras.layers.MaxPooling2D((2,2),strides=2)\n","        self.adv = tf.keras.layers.Dense(1,kernel_initializer=VarianceScaling(scale=2.))\n","        self.val = tf.keras.layers.Dense(num_actions,kernel_initializer=VarianceScaling(scale=2.))\n","        self.drop = tf.keras.layers.Dropout(0.2)\n","        self.cussp = tf.keras.layers.Lambda(lambda w: tf.split(w,2,3))\n","        self.redm = tf.keras.layers.Lambda(lambda w: tf.reduce_mean(w, axis=1, keepdims =True))\n","        self.hidden_layers = [] # List of hidden layers\n","        for i in hidden_units: # Create and append layers to the hidden layer list\n","            self.hidden_layers.append(tf.keras.layers.Dense(\n","            i,activation='relu',kernel_initializer='RandomNormal'))         \n","#         self.output_layer = tf.keras.layers.Dense(\n","#             num_actions,activation='linear', kernel_initializer = 'RandomNormal')\n","        \n","    @tf.function\n","    def call(self, inputs): # Forward passing\n","#         inputs = preprocess(inputs)\n","        z = self.input_layer(inputs) # Assign input to a layer\n","        z = self.conv(z)\n","        z = self.maxp(z)\n","        z = self.conv2(z)\n","        z = self.maxp(z)\n","        z = self.drop(z)\n","        for layer in self.hidden_layers: # Passes the input layer through all of the hidden layers\n","            z = layer(z)\n","        val_stream, adv_stream = self.cussp(z)\n","        val_stream = self.flatten_layer(val_stream)\n","        val = self.val(val_stream)\n","        adv_stream = self.flatten_layer(adv_stream)\n","        adv = self.adv(adv_stream)\n","        q_val = tf.keras.layers.Add()([val, tf.keras.layers.Subtract()([adv, self.redm(adv)])])\n","#         output = self.output_layer(z) # Return the output of the output layer\n","        return q_val\n","    \n","   "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8h9EX_lB3BYb","colab_type":"text"},"source":["# Building DQN Model"]},{"cell_type":"code","metadata":{"id":"eBCUekbG3BYb","colab_type":"code","colab":{}},"source":["class DQN:\n","    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n","        self.num_actions = num_actions # Number of action\n","        self.gamma = gamma # Reward discount factor\n","        self.max_experiences = max_experiences # Max number of exp\n","        self.min_experiences = min_experiences # Min number of exp\n","        self.experience = {'s': [], 'a':[], 'r':[], 's2': [],'done': []} # Exp holder\n","        self.batch_size = batch_size # Batch size\n","        self.optimizer = tf.keras.optimizers.Adam(lr) # Optimizer with learning rate\n","        self.model = Network(num_states, hidden_units,num_actions) # Defining the model\n","        \n","    # Predict the next action with state as input\n","    def predict(self,inputs):\n","        return self.model(atleast_4d(inputs.astype('float32')))\n","    \n","    # Training the model\n","    def train(self, TargetNet):\n","        if len(self.experience['s']) < self.min_experiences: # If there are not enough experience to sample from\n","            return 0\n","        # Randomly sampling from experience (s,s',a,r)\n","        ids = np.random.randint (low=0, high=len(self.experience['s']),size = self.batch_size)\n","        states = np.asarray([self.experience['s'][i] for i in ids])\n","        actions = np.asarray([self.experience['a'][i] for i in ids])\n","        rewards = np.asarray([self.experience['r'][i] for i in ids])\n","        states_next = np.asarray([self.experience['s2'][i] for i in ids])\n","        \n","        # Check for termial state \n","        dones = np.asarray([self.experience['done'][i] for i in ids])\n","        \n","        # Predicting the value of the next state Q(s') by using predict and inputting the next state (s')\n","        value_next = np.max(TargetNet.predict(states_next),axis =1)\n","        actual_values = np.where(dones,rewards,rewards+self.gamma*value_next)\n","\n","        # Calculate the sqared loss between the real target and the prediction values\n","        with tf.GradientTape() as tape: # Recording the computation to compute the differentialtion bakcward (backpropagation)\n","            selected_action_values = tf.math.reduce_sum(\n","                self.predict(states)* tf.one_hot(actions, self.num_actions), axis =1)\n","            l = tf.keras.losses.Huber()\n","            loss = l(actual_values , selected_action_values)\n","        \n","        # Backpropagation     \n","        variables = self.model.trainable_variables # Call the weights of the model\n","        gradients = tape.gradient(loss,variables) # Setup gradients \n","        self.optimizer.apply_gradients(zip(gradients, variables)) # Doing backprop\n","        return loss\n","    \n","    # Epsilon-Greedy Strategy\n","    def get_action(self,states, epsilon):\n","        if np.random.random() < epsilon:\n","            return np.random.choice(self.num_actions) # Exploration\n","        else:\n","            return np.argmax(self.predict(np.atleast_2d(states))[0]) # Exploitation\n","    \n","    # Adding and pushing experience (Replay Memories)\n","    def add_experience(self,exp):\n","        if len(self.experience['s']) >= self.max_experiences:\n","            for key in self.experience.keys():\n","                self.experience[key].pop(0)\n","                \n","        for key,value in exp.items():\n","            self.experience[key].append(value)\n","            \n","    # Copy weights from TrainNet to TargetNet\n","    def copy_weights(self, TrainNet):\n","        variables1 = self.model.trainable_variables\n","        variables2 = TrainNet.model.trainable_variables\n","        for v1, v2 in zip(variables1, variables2):\n","            v1.assign(v2.numpy())\n","    \n","  \n","    def save_model(self):\n","        self.model.save('/content/drive/My Drive/Google Drive/2020/Machine Learning/Jupyter Notebook/Assignment 2/model')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ts_GJx643BYd","colab_type":"text"},"source":["# Play and Save"]},{"cell_type":"code","metadata":{"id":"lso01khN3BYe","colab_type":"code","colab":{}},"source":["def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n","    rewards = 0\n","    iters = 0\n","    done = False\n","    observations = env.reset()\n","    losses = list()\n","    stack_size = 4\n","    stacked_frames = deque([np.zeros((70,70,1),dtype=np.int)for i in range (stack_size)],maxlen =4)\n","    observations,stacked_frames = stack_frames(stacked_frames,observations,True)\n","    while not done:\n","        action = TrainNet.get_action(observations, epsilon)\n","        prev_observations = observations \n","        observations, reward, done,_ = env.step(action)\n","        observations,stacked_frames = stack_frames(stacked_frames,observations,False)\n","        rewards += reward\n","        if env.ale.lives() < 3:\n","            done = True\n","        if done:\n","            reward = -200\n","            env.reset()\n","            \n","        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2':observations, 'done': done}\n","        TrainNet.add_experience(exp)\n","        loss = TrainNet.train(TargetNet)\n","            \n","        # Convert loss to int if loss is not int\n","        if isinstance(loss,int):\n","            losses.append(loss)\n","        else:\n","            losses.append(loss.numpy())\n","            \n","        iters += 1\n","        if iters % copy_step == 0:\n","            TargetNet.copy_weights(TrainNet)\n","    return rewards, np.mean(losses)\n","    \n","def make_video(env,TrainNet):\n","    env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"videos\"), force=True)\n","    rewards = 0\n","    steps = 0\n","    done = False\n","    observations = env.reset()\n","    stack_size = 4\n","    stacked_frames = deque([np.zeros((70,70,1),dtype=np.int)for i in range (stack_size)],maxlen =4)\n","    observations,stacked_frames = stack_frames(stacked_frames,observations,True)\n","    while not done:\n","        action = TrainNet.get_action(observations,0)\n","        observations, reward, done,_ = env.step(action)\n","        observations,stacked_frames = stack_frames(stacked_frames,observations,False)\n","        #env.reset()\n","        steps += 1\n","        rewards += reward\n","    print(\"Testing steps: {} reward {}: \".format(steps,rewards))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bCZOhu-93BYg","colab_type":"text"},"source":["# Main Program"]},{"cell_type":"code","metadata":{"id":"dSWpUqXO3BYh","colab_type":"code","outputId":"926eee26-889d-4c07-9ff0-cde540a7bca6","colab":{"base_uri":"https://localhost:8080/","height":422},"executionInfo":{"status":"error","timestamp":1588650490346,"user_tz":-420,"elapsed":1715,"user":{"displayName":"Phat Phan","photoUrl":"","userId":"18231581227432799548"}}},"source":["env = gym.make('SpaceInvaders-v0')\n","\n","gamma = 0.5\n","copy_step = 50\n","num_states = len(env.observation_space.sample()) \n","num_actions = env.action_space.n\n","hidden_units = [100,100]\n","max_experiences = 1000000\n","min_experiences = 10000\n","batch_size = 32\n","lr = 1e-3\n","current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","log_dir = 'logs/dqn/' + current_time\n","summary_writer = tf.summary.create_file_writer(log_dir)\n","\n","TrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences,batch_size, lr)\n","TargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences,batch_size, lr)\n","\n","N = 10000 # number of episode\n","total_rewards = np.empty(N)\n","max_epsilon = 1\n","decay = 0.001\n","min_epsilon = 0.01\n","for n in range(N):\n","    if n%500 == 0:\n","      TrainNet.save_model()\n","    epsilon = min_epsilon + (max_epsilon-min_epsilon) * np.exp(-decay*n)\n","    total_reward, losses = play_game(env,TrainNet,TargetNet,epsilon,copy_step)\n","    total_rewards[n] = total_reward\n","    avg_rewards = total_rewards[max(0,n-10):(n+1)].mean()\n","    with summary_writer.as_default():\n","        tf.summary.scalar('episode reward', total_reward, step = n)\n","        tf.summary.scalar('running avg reward(10)', avg_rewards, step =n)\n","        tf.summary.scalar('average loss', losses, step=n)\n","    if n%10 ==0:\n","         print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards,\n","                  \"episode loss: \", losses)\n","\n","        \n","print(\"avg reward for last 10 episodes:\", avg_rewards)\n","make_video(env, TrainNet)\n","env.close()"],"execution_count":20,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.Network object at 0x7f3303b84d30>, because it is not built.\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-a981c93aeccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0mTargetNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_epsilon\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_epsilon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmin_epsilon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtotal_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrainNet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTargetNet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcopy_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-9d52ec39a746>\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/Google Drive/2020/Machine Learning/Jupyter Notebook/Assignment 2/model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \"\"\"\n\u001b[1;32m   1051\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1052\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n\u001b[0;32m--> 138\u001b[0;31m                           signatures, options)\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saved_model/save.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(model, filepath, overwrite, include_optimizer, signatures, options)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msave_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_skip_serialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_model_input_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mraise_model_input_error\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0;34m'set. Usually, input shapes are automatically determined from calling'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;34m' .fit() or .predict(). To manually set the shapes, call '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m       'model._set_inputs(inputs).'.format(model))\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Model <__main__.Network object at 0x7f3303b84d30> cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling .fit() or .predict(). To manually set the shapes, call model._set_inputs(inputs)."]}]},{"cell_type":"code","metadata":{"id":"RVXrOFQc6EVE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-uE0pH2-3BYk","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yA9Ex8_03BYn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}